{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic API 101\n",
    "\n",
    "This document talks about the Agentic APIs in Llama Stack. Before you begin, please ensure Llama Stack is installed and set up by following the [Getting Started Guide](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html).\n",
    "\n",
    "Starting Llama 3.1 you can build agentic applications capable of:\n",
    "\n",
    "- breaking a task down and performing multi-step reasoning.\n",
    "- using tools to perform some actions\n",
    "  - built-in: the model has built-in knowledge of tools like search or code interpreter\n",
    "  - zero-shot: the model can learn to call tools using previously unseen, in-context tool definitions\n",
    "- providing system level safety protections using models like Llama Guard.\n",
    "\n",
    "An agentic app requires a few components:\n",
    "- ability to run inference on the underlying Llama series of models\n",
    "- ability to run safety checks using the Llama Guard series of models\n",
    "- ability to execute tools, including a code execution environment, and loop using the model's multi-step reasoning process\n",
    "\n",
    "All of these components are now offered by a single Llama Stack Distribution. Llama Stack defines and standardizes these components and many others that are needed to make building Generative AI applications smoother. Various implementations of these APIs are then assembled together via a **Llama Stack Distribution**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Agent example\n",
    "\n",
    "Please check out examples with client SDKs to talk with the Llama Stack server in our [llama-stack-apps](https://github.com/meta-llama/llama-stack-apps) repo. \n",
    "\n",
    "In this tutorial, with the `Llama3.1-8B-Instruct` server running, we can use the following code to run a simple agent example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your connection parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"localhost\"  # Replace with your host\n",
    "PORT = 5000        # Replace with your port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'llama_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m EventLogger()\u001b[38;5;241m.\u001b[39mlog(response):\n\u001b[1;32m     60\u001b[0m             log\u001b[38;5;241m.\u001b[39mprint()\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m agent_example()\n",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m, in \u001b[0;36magent_example\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models_response:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39midentifier\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstruct\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m         model_name \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_model\u001b[49m\n\u001b[1;32m     15\u001b[0m agent_config \u001b[38;5;241m=\u001b[39m AgentConfig(\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m     17\u001b[0m     instructions\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     enable_session_persistence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(client, agent_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/hack/lib/python3.11/site-packages/pydantic/main.py:896\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 896\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'llama_model'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "from llama_stack_client.types.agent_create_params import AgentConfig\n",
    "\n",
    "os.environ[\"BRAVE_SEARCH_API_KEY\"] = \"YOUR_SEARCH_API_KEY\"\n",
    "\n",
    "async def agent_example():\n",
    "    client = LlamaStackClient(base_url=f\"http://{HOST}:{PORT}\")\n",
    "    models_response = client.models.list()\n",
    "    for model in models_response:\n",
    "        if model.identifier.endswith(\"Instruct\"):\n",
    "            model_name = model.llama_model\n",
    "    agent_config = AgentConfig(\n",
    "        model=model_name,\n",
    "        instructions=\"You are a helpful assistant\",\n",
    "        sampling_params={\n",
    "            \"strategy\": \"greedy\",\n",
    "            \"temperature\": 1.0,\n",
    "            \"top_p\": 0.9,\n",
    "        },\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"brave_search\",\n",
    "                \"engine\": \"brave\",\n",
    "                \"api_key\": os.getenv(\"BRAVE_SEARCH_API_KEY\"),\n",
    "            }\n",
    "        ],\n",
    "        tool_choice=\"auto\",\n",
    "        tool_prompt_format=\"function_tag\",\n",
    "        input_shields=[],\n",
    "        output_shields=[],\n",
    "        enable_session_persistence=False,\n",
    "    )\n",
    "\n",
    "    agent = Agent(client, agent_config)\n",
    "    session_id = agent.create_session(\"test-session\")\n",
    "    print(f\"Created session_id={session_id} for Agent({agent.agent_id})\")\n",
    "\n",
    "    user_prompts = [\n",
    "        \"I am planning a trip to Switzerland, what are the top 3 places to visit?\",\n",
    "        \"What is so special about #1?\",\n",
    "        \"What other countries should I consider to club?\",\n",
    "        \"What is the capital of France?\",\n",
    "    ]\n",
    "\n",
    "    for prompt in user_prompts:\n",
    "        response = agent.create_turn(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            session_id=session_id,\n",
    "        )\n",
    "\n",
    "        async for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "\n",
    "\n",
    "await agent_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have come a long way from getting started to understanding the internals of Llama-Stack! \n",
    "\n",
    "Thanks for joining us on this journey. If you have questions-please feel free to open an issue. Looking forward to what you build with Open Source AI!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
